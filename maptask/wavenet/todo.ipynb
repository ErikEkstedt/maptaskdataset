{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Todo\n",
    "\n",
    "## Data\n",
    "\n",
    "1. Extract f0, pitch and melspectrogram\n",
    "2. Time and frame\n",
    "matching\n",
    "3. Parallel structure like r9r9\n",
    "4. Pull request to add maptask to\n",
    "wavenet pipeline\n",
    "\n",
    "\n",
    "## Training\n",
    "1. Training\n",
    "2. Smaller network on pitch and f0\n",
    "3.\n",
    "Experiment: \n",
    "\t- Better experience/more natural with mimicking pitch and f0 than\n",
    "without\n",
    "\n",
    "-----------------------\n",
    "\n",
    "# Code\n",
    "\n",
    "Extract pitch and f0 information on frame basis\n",
    "\n",
    "Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import os\n",
    "import audio\n",
    "\n",
    "from nnmnkwii import preprocessing as P\n",
    "from hparams import hparams\n",
    "from os.path import exists\n",
    "import librosa\n",
    "\n",
    "from wavenet_vocoder.util import is_mulaw_quantize, is_mulaw, is_raw\n",
    "\n",
    "\n",
    "def build_from_path(in_dir, out_dir, num_workers=1, tqdm=lambda x: x):\n",
    "    executor = ProcessPoolExecutor(max_workers=num_workers)\n",
    "    futures = []\n",
    "    index = 1\n",
    "    with open(os.path.join(in_dir, 'metadata.csv'), encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split('|')\n",
    "            wav_path = os.path.join(in_dir, 'wavs', '%s.wav' % parts[0])\n",
    "            text = parts[2]\n",
    "            futures.append(executor.submit(\n",
    "                partial(_process_utterance, out_dir, index, wav_path, text)))\n",
    "            index += 1\n",
    "    return [future.result() for future in tqdm(futures)]\n",
    "\n",
    "\n",
    "def _process_utterance(out_dir, index, wav_path, text):\n",
    "    # Load the audio to a numpy array:\n",
    "    wav = audio.load_wav(wav_path)\n",
    "\n",
    "    if hparams.rescaling:\n",
    "        wav = wav / np.abs(wav).max() * hparams.rescaling_max\n",
    "\n",
    "    # Mu-law quantize\n",
    "    if is_mulaw_quantize(hparams.input_type):\n",
    "        # [0, quantize_channels)\n",
    "        out = P.mulaw_quantize(wav, hparams.quantize_channels)\n",
    "\n",
    "        # Trim silences\n",
    "        start, end = audio.start_and_end_indices(out, hparams.silence_threshold)\n",
    "        wav = wav[start:end]\n",
    "        out = out[start:end]\n",
    "        constant_values = P.mulaw_quantize(0, hparams.quantize_channels)\n",
    "        out_dtype = np.int16\n",
    "    elif is_mulaw(hparams.input_type):\n",
    "        # [-1, 1]\n",
    "        out = P.mulaw(wav, hparams.quantize_channels)\n",
    "        constant_values = P.mulaw(0.0, hparams.quantize_channels)\n",
    "        out_dtype = np.float32\n",
    "    else:\n",
    "        # [-1, 1]\n",
    "        out = wav\n",
    "        constant_values = 0.0\n",
    "        out_dtype = np.float32\n",
    "\n",
    "    # Compute a mel-scale spectrogram from the trimmed wav:\n",
    "    # (N, D)\n",
    "    mel_spectrogram = audio.melspectrogram(wav).astype(np.float32).T\n",
    "    # lws pads zeros internally before performing stft\n",
    "    # this is needed to adjust time resolution between audio and mel-spectrogram\n",
    "    l, r = audio.lws_pad_lr(wav, hparams.fft_size, audio.get_hop_size())\n",
    "\n",
    "    # zero pad for quantized signal\n",
    "    out = np.pad(out, (l, r), mode=\"constant\", constant_values=constant_values)\n",
    "    N = mel_spectrogram.shape[0]\n",
    "    assert len(out) >= N * audio.get_hop_size()\n",
    "\n",
    "    # time resolution adjustment\n",
    "    # ensure length of raw audio is multiple of hop_size so that we can use\n",
    "    # transposed convolution to upsample\n",
    "    out = out[:N * audio.get_hop_size()]\n",
    "    assert len(out) % audio.get_hop_size() == 0\n",
    "\n",
    "    timesteps = len(out)\n",
    "\n",
    "    # Write the spectrograms to disk:\n",
    "    audio_filename = 'ljspeech-audio-%05d.npy' % index\n",
    "    mel_filename = 'ljspeech-mel-%05d.npy' % index\n",
    "    np.save(os.path.join(out_dir, audio_filename),\n",
    "            out.astype(out_dtype), allow_pickle=False)\n",
    "    np.save(os.path.join(out_dir, mel_filename),\n",
    "            mel_spectrogram.astype(np.float32), allow_pickle=False)\n",
    "\n",
    "    # Return a tuple describing this training example:\n",
    "    return (audio_filename, mel_filename, timesteps, text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source [audio.py](https://github.com/r9y9/wavenet_vocoder/blob/master/audio.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.filters\n",
    "import math\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "from hparams import hparams\n",
    "from scipy.io import wavfile\n",
    "\n",
    "import lws\n",
    "\n",
    "\n",
    "def load_wav(path):\n",
    "    return librosa.core.load(path, sr=hparams.sample_rate)[0]\n",
    "\n",
    "\n",
    "def save_wav(wav, path):\n",
    "    wav *= 32767 / max(0.01, np.max(np.abs(wav)))\n",
    "    wavfile.write(path, hparams.sample_rate, wav.astype(np.int16))\n",
    "\n",
    "\n",
    "def trim(quantized):\n",
    "    start, end = start_and_end_indices(quantized, hparams.silence_threshold)\n",
    "    return quantized[start:end]\n",
    "\n",
    "\n",
    "def adjust_time_resolution(quantized, mel):\n",
    "    \"\"\"Adjust time resolution by repeating features\n",
    "\n",
    "    Args:\n",
    "        quantized (ndarray): (T,)\n",
    "        mel (ndarray): (N, D)\n",
    "\n",
    "    Returns:\n",
    "        tuple: Tuple of (T,) and (T, D)\n",
    "    \"\"\"\n",
    "    assert len(quantized.shape) == 1\n",
    "    assert len(mel.shape) == 2\n",
    "\n",
    "    upsample_factor = quantized.size // mel.shape[0]\n",
    "    mel = np.repeat(mel, upsample_factor, axis=0)\n",
    "    n_pad = quantized.size - mel.shape[0]\n",
    "    if n_pad != 0:\n",
    "        assert n_pad > 0\n",
    "        mel = np.pad(mel, [(0, n_pad), (0, 0)], mode=\"constant\", constant_values=0)\n",
    "\n",
    "    # trim\n",
    "    start, end = start_and_end_indices(quantized, hparams.silence_threshold)\n",
    "\n",
    "    return quantized[start:end], mel[start:end, :]\n",
    "adjast_time_resolution = adjust_time_resolution  # 'adjust' is correct spelling, this is for compatibility\n",
    "\n",
    "\n",
    "def start_and_end_indices(quantized, silence_threshold=2):\n",
    "    for start in range(quantized.size):\n",
    "        if abs(quantized[start] - 127) > silence_threshold:\n",
    "            break\n",
    "    for end in range(quantized.size - 1, 1, -1):\n",
    "        if abs(quantized[end] - 127) > silence_threshold:\n",
    "            break\n",
    "\n",
    "    assert abs(quantized[start] - 127) > silence_threshold\n",
    "    assert abs(quantized[end] - 127) > silence_threshold\n",
    "\n",
    "    return start, end\n",
    "\n",
    "\n",
    "def melspectrogram(y):\n",
    "    D = _lws_processor().stft(y).T\n",
    "    S = _amp_to_db(_linear_to_mel(np.abs(D))) - hparams.ref_level_db\n",
    "    if not hparams.allow_clipping_in_normalization:\n",
    "        assert S.max() <= 0 and S.min() - hparams.min_level_db >= 0\n",
    "    return _normalize(S)\n",
    "\n",
    "\n",
    "def get_hop_size():\n",
    "    hop_size = hparams.hop_size\n",
    "    if hop_size is None:\n",
    "        assert hparams.frame_shift_ms is not None\n",
    "        hop_size = int(hparams.frame_shift_ms / 1000 * hparams.sample_rate)\n",
    "    return hop_size\n",
    "\n",
    "\n",
    "def _lws_processor():\n",
    "    return lws.lws(hparams.fft_size, get_hop_size(), mode=\"speech\")\n",
    "\n",
    "\n",
    "def lws_num_frames(length, fsize, fshift):\n",
    "    \"\"\"Compute number of time frames of lws spectrogram\n",
    "    \"\"\"\n",
    "    pad = (fsize - fshift)\n",
    "    if length % fshift == 0:\n",
    "        M = (length + pad * 2 - fsize) // fshift + 1\n",
    "    else:\n",
    "        M = (length + pad * 2 - fsize) // fshift + 2\n",
    "    return M\n",
    "\n",
    "\n",
    "def lws_pad_lr(x, fsize, fshift):\n",
    "    \"\"\"Compute left and right padding lws internally uses\n",
    "    \"\"\"\n",
    "    M = lws_num_frames(len(x), fsize, fshift)\n",
    "    pad = (fsize - fshift)\n",
    "    T = len(x) + 2 * pad\n",
    "    r = (M - 1) * fshift + fsize - T\n",
    "    return pad, pad + r\n",
    "\n",
    "# Conversions:\n",
    "\n",
    "\n",
    "_mel_basis = None\n",
    "\n",
    "\n",
    "def _linear_to_mel(spectrogram):\n",
    "    global _mel_basis\n",
    "    if _mel_basis is None:\n",
    "        _mel_basis = _build_mel_basis()\n",
    "    return np.dot(_mel_basis, spectrogram)\n",
    "\n",
    "\n",
    "def _build_mel_basis():\n",
    "    assert hparams.fmax <= hparams.sample_rate // 2\n",
    "    return librosa.filters.mel(hparams.sample_rate, hparams.fft_size,\n",
    "                               fmin=hparams.fmin, fmax=hparams.fmax,\n",
    "                               n_mels=hparams.num_mels)\n",
    "\n",
    "\n",
    "def _amp_to_db(x):\n",
    "    min_level = np.exp(hparams.min_level_db / 20 * np.log(10))\n",
    "    return 20 * np.log10(np.maximum(min_level, x))\n",
    "\n",
    "\n",
    "def _db_to_amp(x):\n",
    "    return np.power(10.0, x * 0.05)\n",
    "\n",
    "\n",
    "def _normalize(S):\n",
    "    return np.clip((S - hparams.min_level_db) / -hparams.min_level_db, 0, 1)\n",
    "\n",
    "\n",
    "def _denormalize(S):\n",
    "    return (np.clip(S, 0, 1) * -hparams.min_level_db) + hparams.min_level_db"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
