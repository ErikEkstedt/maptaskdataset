{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming\n",
    "\n",
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "1"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "from tqdm import tqdm\n",
    "from scipy.io.wavfile import read, write\n",
    "import xml.etree.ElementTree as ET\n",
    "from librosa import time_to_samples as librosa_time_to_samples\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchaudio.transforms import MEL2 as MelSpectrogram\n",
    "from torchaudio.transforms import SPECTROGRAM as Spec\n",
    "\n",
    "from utils import get_paths, load_audio, visualize_backchannel, sound_backchannel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "2"
    }
   },
   "outputs": [],
   "source": [
    "def get_paths(root_path=None):\n",
    "    # Assumes maptask dialogue files are downloaded in $dataPath\n",
    "    if not root_path:\n",
    "        try:\n",
    "            full_path = os.path.realpath(__file__)\n",
    "            root_path, filename = os.path.split(full_path)\n",
    "        except: # for ipython repl error\n",
    "            print('Assumes this repo is in home directory')\n",
    "            root_path = join(os.path.expanduser('~'), 'maptaskdataset')\n",
    "    data_path = os.path.realpath(join(root_path, 'data'))\n",
    "    return {'data_path' : data_path,\n",
    "            'annotation_path' : join(data_path, 'maptaskv2-1'),\n",
    "            'dialog_path' : join(data_path, 'dialogues'),\n",
    "            'mono_path' : join(data_path, 'dialogues_mono'),\n",
    "            'timed_units_path': join(data_path, \"maptaskv2-1/Data/timed-units\"),\n",
    "            'gemap_path' : join(data_path, 'gemaps'),\n",
    "            'opensmile_path' : join(os.path.expanduser('~'), 'opensmile-2.3.0')}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths\n",
    "Useful script to load a custom data path or generic which I try to use\n",
    "across systems.\n",
    "\n",
    "This one is helpful and could be booilerplate in all code.\n",
    "A\n",
    "function that could use a custom path or go to standard.\n",
    "\n",
    "## Maptask\n",
    "\n",
    "This class\n",
    "iterates through the annotations provided by maptask and extracts\n",
    "backchannel\n",
    "uttterences.\n",
    "\n",
    "The annotations in the dataset contains words spoken and\n",
    "correlating timings.\n",
    "Utterences from one speaker seperated by less than `pause`\n",
    "are combined to make\n",
    "a longer utterence, a sentence.\n",
    "\n",
    "All utterences which only\n",
    "contains one word are extracted and sorted by\n",
    "popularity. From this list of one\n",
    "word utterences the most common ones which are\n",
    "known to be back channels are\n",
    "then extracted.\n",
    "\n",
    "As of now this means that the 6 words below make up all\n",
    "utterences in the\n",
    "dataset:\n",
    "\n",
    "back_channels = ['right', 'okay', 'mmhmm', 'uh-huh',\n",
    "'yeah', 'mm']\n",
    "\n",
    "This class stores all extracted backchannels in a list where each\n",
    "entry in the\n",
    "list is a dict[ 'name', 'time', 'sample', 'words' ]\n",
    "\n",
    "Here 'name' is\n",
    "the name of the session, 'time' (seconds) and 'sample'(array\n",
    "index) are the\n",
    "timings of the utterence and 'words' is a list of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "3"
    }
   },
   "outputs": [],
   "source": [
    "class Maptask(object):\n",
    "    '''\n",
    "    This class iterates through the annotations provided by maptask and extracts\n",
    "    backchannel uttterences.\n",
    "\n",
    "    The annotations in the dataset contains words spoken and correlating timings.\n",
    "    Utterences from one speaker seperated by less than `pause` are combined to\n",
    "    make a longer utterence, a sentence.\n",
    "\n",
    "    All utterences which only contains one word are extracted and sorted by\n",
    "    popularity. From this list of one word utterences the most common ones which\n",
    "    are known to be back channels are then extracted.\n",
    "\n",
    "    As of now this means that the 6 words below make up all utterences in the\n",
    "    dataset:\n",
    "\n",
    "    self.back_channels = ['right', 'okay', 'mmhmm', 'uh-huh', 'yeah', 'mm']\n",
    "\n",
    "    This class stores all extracted backchannels in a list where each entry in\n",
    "    the list is a dict:\n",
    "\n",
    "        {'name': name, 'time', time, 'sample': sample, 'words': words}\n",
    "\n",
    "    Here 'name' is the name of the session, 'time' and 'sample' are the timings of\n",
    "    the utterence and 'words' is a list of strings.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, pause=0.2, pre_padding=0, post_padding=0, max_utterences=1, root=None):\n",
    "        self.paths = get_paths(root)\n",
    "        self.session_names = self._session_names()\n",
    "\n",
    "        self.pause = pause\n",
    "        self.pre_padding = pre_padding\n",
    "        self.post_padding = post_padding\n",
    "        self.max_utterences = max_utterences\n",
    "        self.back_channels = ['right', 'okay', 'mmhmm', 'uh-huh', 'yeah', 'mm']\n",
    "\n",
    "        f_data, g_data = self.extract_all_short_utterence_from_both_users()\n",
    "        self.f_utter, self.f_vocab = self.get_utterences(f_data)\n",
    "        self.g_utter, self.g_vocab = self.get_utterences(g_data)\n",
    "\n",
    "        self.back_channel_list = self.get_back_channel_list()\n",
    "        # self.add_audio_signal_to_back_channel_data()\n",
    "\n",
    "    def _session_names(self):\n",
    "        return [fname.split('.')[0] for fname in \\\n",
    "                os.listdir(self.paths['dialog_path']) if fname.endswith('.wav')]\n",
    "\n",
    "    def extract_tag_data_from_xml_path(self, xml_path=None):\n",
    "        '''\n",
    "        Extract timed-unit (tu), silence (si) and noise (noi) tags from the\n",
    "        .xml annotation file.\n",
    "        '''\n",
    "\n",
    "        if not xml_path:\n",
    "            raise OSError(\"xml path required\")\n",
    "\n",
    "        # parse xml\n",
    "        xml_element_tree = ET.parse(xml_path)\n",
    "\n",
    "        tu, words, sil, noi = [], [], [], []\n",
    "        for elem in xml_element_tree.iter():\n",
    "            try:\n",
    "                tmp = (float(elem.attrib['start']), float(elem.attrib['end']))\n",
    "            except:\n",
    "                continue\n",
    "            if elem.tag == 'tu':\n",
    "                # elem.attrib: start, end, utt\n",
    "                words.append(elem.text)  # word annotation\n",
    "                tu.append(tmp)\n",
    "            elif elem.tag == 'sil':\n",
    "                # elem.attrib: start, end\n",
    "                sil.append(tmp)\n",
    "            elif elem.tag == 'noi':\n",
    "                # elem.attrib: start, end, type='outbreath/lipsmack/...'\n",
    "                noi.append(tmp)\n",
    "\n",
    "        return {'tu': tu, 'silence': sil, 'noise': noi, 'words': words}\n",
    "\n",
    "    def merge_pauses(self, tu, words):\n",
    "        new_tu, new_words = [], []\n",
    "        start, last_end, tmp_words = 0, 0, []\n",
    "        for i, (t, w) in enumerate(zip(tu, words)):\n",
    "            # t[0] - start,  t[1] - end\n",
    "            pause_duration = t[0] - last_end\n",
    "            if pause_duration > self.pause:\n",
    "                new_tu.append((start, last_end))\n",
    "                new_words.append(tmp_words)\n",
    "                tmp_words = [w]\n",
    "                start = t[0]\n",
    "                last_end = t[1]\n",
    "            else:\n",
    "                tmp_words.append(w)\n",
    "                last_end = t[1]\n",
    "        return new_tu[1:], new_words[1:]  # remove first entry which is always zero\n",
    "\n",
    "    def get_timing_utterences(self, name, user='f'):\n",
    "\n",
    "        # load timed-units.xml. Searching through dir.\n",
    "        for file in os.listdir(self.paths['timed_units_path']):\n",
    "            if name in file:\n",
    "                if '.'+user+'.' in file:\n",
    "                    xml_path = join(self.paths['timed_units_path'], file)\n",
    "\n",
    "        data = self.extract_tag_data_from_xml_path(xml_path)\n",
    "\n",
    "        times, words = self.merge_pauses(data['tu'], data['words'])\n",
    "\n",
    "        samples = librosa_time_to_samples(times, sr=20000)\n",
    "\n",
    "        return [{'name': name, 'user': user, 'time':time, 'sample': sample, 'words': word} \\\n",
    "                for time, sample, word in zip(times, samples, words)]\n",
    "\n",
    "    def extract_all_short_utterence_from_both_users(self):\n",
    "        f_data, g_data = [], []\n",
    "        for name in tqdm(self.session_names):\n",
    "            f_data.append(self.get_timing_utterences(name, user='f'))\n",
    "            g_data.append(self.get_timing_utterences(name, user='g'))\n",
    "        return f_data, g_data\n",
    "\n",
    "    def get_utterences(self, data):\n",
    "        utterence_data, vocab = [], {}\n",
    "        for session in data:\n",
    "            tmp_session_data = []\n",
    "            for utterence in session:\n",
    "                utter = utterence['words']\n",
    "                if len(utter) <= self.max_utterences:\n",
    "                    tmp_session_data.append(utterence)\n",
    "                    if not utter[0] in vocab.keys():\n",
    "                        vocab[utter[0]] = 1\n",
    "                    else:\n",
    "                        vocab[utter[0]] += 1\n",
    "            utterence_data.append(tmp_session_data)\n",
    "\n",
    "        vocab = sorted(vocab.items(), key=lambda t: t[1], reverse=True)\n",
    "        return utterence_data, vocab\n",
    "\n",
    "    def get_back_channel_list(self):\n",
    "        back_channel_list = []\n",
    "        for file_utters in self.f_utter + self.g_utter:\n",
    "            for utter in file_utters:\n",
    "                word = utter['words'][0]\n",
    "                if word in self.back_channels:\n",
    "                    back_channel_list.append(utter)\n",
    "        return back_channel_list\n",
    "\n",
    "    def print_vocab(self, top=5):\n",
    "        f_dpoints, f_back = 0, []\n",
    "        g_dpoints, g_back = 0, []\n",
    "        for i in range(top):\n",
    "            f_back.append(self.f_vocab[i][0])\n",
    "            f_dpoints += self.f_vocab[i][1]\n",
    "            g_back.append(self.g_vocab[i][0])\n",
    "            g_dpoints += self.g_vocab[i][1]\n",
    "        print('Guide:')\n",
    "        print('Datapoints: ', g_dpoints)\n",
    "        print('Vocab: ', g_back)\n",
    "        print()\n",
    "        print('Follower:')\n",
    "        print('Datapoints: ', f_dpoints)\n",
    "        print('Vocab: ', f_back)\n",
    "        print()\n",
    "        print('Total: ', g_dpoints + f_dpoints)\n",
    "        print('-'*50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maptask\n",
    "\n",
    "This dataset goes through the annotations and extract relevant\n",
    "backchannels.\n",
    "\n",
    "A pause of 0.1 seconds means that all utterences seperated by\n",
    "less than 0.1 seconds are\n",
    "concatonated into longer utterences. Then"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pause = 0.1\n",
    "max_utterences = 1\n",
    "\n",
    "maptask = Maptask(pause, max_utterences)\n",
    "\n",
    "print(len(maptask.back_channel_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "The PyTorch Dataset class uses the maptask class above in order to\n",
    "get the\n",
    "relevant annotations. First all the audio is loaded from the\n",
    "maptask.paths['dialog\\_path'] for faster extraction during training.\n",
    "\n",
    "The audio\n",
    "is loaded using `torchaudio.load` and if `normalize_audio=True` (True by\n",
    "default) then the audio is normalized by dividing by the maximum value in the\n",
    "audio array (relative normalization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "4"
    }
   },
   "outputs": [],
   "source": [
    "class MaptaskDataset(Dataset):\n",
    "    # 128 dialogs, 16-bit samples, 20 kHz sample rate, 2 channels per conversation\n",
    "    def __init__(self,\n",
    "                 pause=0.5,\n",
    "                 pre_padding=0,\n",
    "                 post_padding=0,\n",
    "                 max_utterences=1,\n",
    "                 sample_rate=20000,\n",
    "                 window_size=50,\n",
    "                 hop_length=None,\n",
    "                 n_fft=None,\n",
    "                 pad=0,\n",
    "                 n_mels=40,\n",
    "                 torch_load_audio=True,\n",
    "                 audio=None,\n",
    "                 normalize_audio=True,\n",
    "                 root=None):\n",
    "        self.maptask = Maptask(pause, pre_padding, post_padding, max_utterences, root)\n",
    "        self.paths = self.maptask.paths\n",
    "\n",
    "        # Audio\n",
    "        self.normalize_audio = normalize_audio\n",
    "        self.pre_padding = librosa_time_to_samples(pre_padding, sr=sample_rate)\n",
    "        self.post_padding = librosa_time_to_samples(post_padding, sr=sample_rate)\n",
    "\n",
    "        self.torch_load_audio = torch_load_audio\n",
    "        if audio:\n",
    "            self.audio = audio\n",
    "        else:\n",
    "            self.audio = load_audio(self.paths['dialog_path'],\n",
    "                                    self.torch_load_audio,\n",
    "                                    self.normalize_audio)\n",
    "\n",
    "        # Mel\n",
    "        self.sample_rate = sample_rate\n",
    "        self.window_size = window_size\n",
    "        self.hop_length = hop_length\n",
    "        self.n_fft = n_fft\n",
    "        self.pad = pad\n",
    "        self.n_mels = n_mels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.maptask.back_channel_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        bc = self.maptask.back_channel_list[idx]\n",
    "        start, end = bc['sample']\n",
    "\n",
    "        # transform time-padding -> sample-padding and add to start, end\n",
    "        start -= librosa_time_to_samples(self.pre_padding, sr=self.sample_rate)\n",
    "        end += librosa_time_to_samples(self.post_padding, sr=self.sample_rate)\n",
    "\n",
    "        # TODO\n",
    "        # Should also extract words of speaker not only backchannel\n",
    "        y = self.audio[bc['name']]\n",
    "        if bc['user'] == 'f':\n",
    "            speaker = y[start:end, 0]\n",
    "            back_channel = y[start:end,1]\n",
    "        else:\n",
    "            speaker = y[1, start:end]\n",
    "            back_channel = y[0, start:end]\n",
    "\n",
    "        # TODO\n",
    "        # Make audio torch.Tensor (quantify)\n",
    "        # Spectrogram\n",
    "        # mel_spec = MelSpectrogram(sr=self.sample_rate,\n",
    "        #                           ws=self.window_size,\n",
    "        #                           hop=self.hop_length,\n",
    "        #                           n_fft=self.n_fft,\n",
    "        #                           pad=self.pad,\n",
    "        #                           n_mels=self.n_mels)\n",
    "\n",
    "        # s = mel_spec(torch.tensor(speaker)\n",
    "        # print(speaker.shape)\n",
    "        # print(back_channel.shape)\n",
    "        # print(s.shape)\n",
    "        # print(mel_spec.shape)\n",
    "        return speaker, back_channel, bc['words']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a dataset where utterances separated by less then are concatenated and\n",
    "utterances containing one word and that word is in `back_channel_list` it counts\n",
    "as a backchannel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "5"
    }
   },
   "outputs": [],
   "source": [
    "dset = MaptaskDataset(pause=0.1, max_utterences=1, normalize_audio=True)\n",
    "\n",
    "print('Amount of backchannels: ', len(dset))\n",
    "\n",
    "speaker, bc, bc_word = dset[8]\n",
    "print('speaker: ', len(speaker), type(speaker))\n",
    "print('backchannel audio: ', len(bc), type(bc))\n",
    "print('word: ', bc_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets visualize the current datapoint and listen to the audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import visualize_backchannel, sound_backchannel\n",
    "\n",
    "visualize_backchannel(speaker, bc, pause=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sound_backchannel(speaker, bc)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
